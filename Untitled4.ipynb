{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "007b68a3-e727-43bc-bb8c-02f56190137c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# NEWS Collection through web scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00392418-662e-4368-bee0-86567dd10b92",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Collecting NEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b53e6b2-cdb7-45ff-a380-db4d4aa76bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "url = \"https://archives.ndtv.com/articles/2020-11.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    news_links = soup.find_all('a')\n",
    "\n",
    "    # Skip the first 40 links\n",
    "    start_index = 50\n",
    "    news_links = news_links[start_index:]\n",
    "\n",
    "    # Limit the number of outputs\n",
    "    num_outputs_to_display = 8000\n",
    "\n",
    "    # CSV file setup\n",
    "    csv_file_path = \"newss_data.csv\"\n",
    "    with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        fieldnames = ['Link Number', 'News Link', 'Description', 'Headline', 'Author', 'Date', 'Paragraph']\n",
    "        csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "\n",
    "        csv_writer.writeheader()\n",
    "\n",
    "        for i, link in enumerate(news_links[:num_outputs_to_display], start=start_index + 1):\n",
    "            news_link = link.get('href')\n",
    "            description = link.text.strip()\n",
    "\n",
    "            # Check if the link is valid\n",
    "            if is_valid_url(news_link):\n",
    "                # Print the information\n",
    "                print(f\"Processing Link {i}: {news_link}\\nDescription: {description}\\n\")\n",
    "\n",
    "                # Retrieve details from the linked page\n",
    "                linked_page_response = requests.get(news_link)\n",
    "                if linked_page_response.status_code == 200:\n",
    "                    linked_page_soup = BeautifulSoup(linked_page_response.content, 'html.parser')\n",
    "\n",
    "                    # Extract details\n",
    "                    headline_element = linked_page_soup.find('h1', itemprop='headline')\n",
    "                    author_element = linked_page_soup.select_one('div.pst-by_ul span.pst-by_li span[itemprop=\"author\"]')\n",
    "                    date_element = linked_page_soup.select_one('div.pst-by_ul span.pst-by_li:not([itemprop])')\n",
    "\n",
    "                    paragraphs = [p.text.strip() for p in linked_page_soup.find_all('p')]\n",
    "\n",
    "                    # Check if elements are found before accessing their properties\n",
    "                    headline = headline_element.text.strip() if headline_element else \"N/A\"\n",
    "                    author = author_element.text.strip() if author_element else \"N/A\"\n",
    "                    date = date_element.text.strip() if date_element else \"N/A\"\n",
    "\n",
    "                    # Print details\n",
    "                    print(f\"Headline: {headline}\\nAuthor: {author}\\nDate: {date}\\nParagraphs:\\n{paragraphs}\\n\")\n",
    "\n",
    "                    # Write to CSV file\n",
    "                    csv_writer.writerow({\n",
    "                        'Link Number': i,\n",
    "                        'News Link': news_link,\n",
    "                        'Description': description,\n",
    "                        'Headline': headline,\n",
    "                        'Author': author,\n",
    "                        'Date': date,\n",
    "                        'Paragraph': paragraphs\n",
    "                    })\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve linked page. Status Code: {linked_page_response.status_code}\\n\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Invalid URL. Skipping link {i}\\n\")\n",
    "\n",
    "    print(f\"\\nData has been saved in the CSV file: {csv_file_path}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570806b1-ed8d-4aa5-a19b-a5155c385b6a",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2633487-9b2c-43c2-9263-cd8caf4ad3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from langdetect import detect\n",
    "\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_non_empty_paragraph(p):\n",
    "    return p.text.strip() if p.text.strip() else None\n",
    "\n",
    "def extract_author(author_element):\n",
    "    if author_element.name == 'a':\n",
    "        return author_element.text.strip()\n",
    "    elif author_element.find('a'):\n",
    "        # Extract text from anchor tag inside the span\n",
    "        return author_element.find('a').text.strip()\n",
    "    else:\n",
    "        return \"N/A\"\n",
    "\n",
    "url = \"https://archives.ndtv.com/articles/2021-04.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    news_links = soup.find_all('a')\n",
    "\n",
    "    # Skip the first 40 links\n",
    "    start_index = 2000\n",
    "    news_links = news_links[start_index:]\n",
    "\n",
    "    # Limit the number of outputs\n",
    "    num_outputs_to_display = 4000\n",
    "\n",
    "    # Limit the number of paragraphs for description\n",
    "    \n",
    "\n",
    "    # CSV file setup for Hindi and English news\n",
    "    csv_file_path_all = \"news_data21_alls.csv\"\n",
    "    fieldnames = ['Link Number', 'News Link', 'Language', 'Description', 'Headline', 'Author', 'Date', 'Paragraph']\n",
    "\n",
    "    with open(csv_file_path_all, 'a', newline='', encoding='utf-8') as csv_file_all:\n",
    "        csv_writer_all = csv.DictWriter(csv_file_all, fieldnames=fieldnames)\n",
    "\n",
    "        # If the file is empty, write the header\n",
    "        if csv_file_all.tell() == 0:\n",
    "            csv_writer_all.writeheader()\n",
    "\n",
    "        for i, link in enumerate(news_links[:num_outputs_to_display], start=start_index + 1):\n",
    "            news_link = link.get('href')\n",
    "            description = link.text.strip()\n",
    "\n",
    "            # Check if the link is valid\n",
    "            if is_valid_url(news_link):\n",
    "                # Print the information\n",
    "                print(f\"Processing Link {i}: {news_link}\\nDescription: {description}\\n\")\n",
    "\n",
    "                # Retrieve details from the linked page\n",
    "                linked_page_response = requests.get(news_link)\n",
    "                if linked_page_response.status_code == 200:\n",
    "                    linked_page_soup = BeautifulSoup(linked_page_response.content, 'html.parser')\n",
    "\n",
    "                    # Extract details\n",
    "                    headline_element = linked_page_soup.find('h1', itemprop='headline')\n",
    "                    author_elements = linked_page_soup.find_all('span', itemprop='author')\n",
    "                    published_date = linked_page_soup.select_one('.pst-by_li:contains(\"Updated:\")')\n",
    "                    date = published_date.text.replace(\"Updated:\", \"\").strip() if published_date else \"N/A\"\n",
    "\n",
    "                    # Exclude paragraphs within the specified div with class \"ft-social\"\n",
    "                    excluded_div = linked_page_soup.find('div', class_='ft-social')\n",
    "                    excluded_paragraphs = set(excluded_div.find_all('p')) if excluded_div else set()\n",
    "\n",
    "                    # Limit the number of paragraphs for description\n",
    "                    description_paragraphs = [\n",
    "                        is_non_empty_paragraph(p)\n",
    "                        for p in linked_page_soup.find_all('p')\n",
    "                        if p not in excluded_paragraphs\n",
    "                    ]\n",
    "\n",
    "                    if any(description_paragraphs):\n",
    "                        headline = headline_element.text.strip() if headline_element else \"N/A\"\n",
    "                        authors = [extract_author(author) for author in author_elements]\n",
    "                        author = ', '.join(authors) if authors else \"N/A\"\n",
    "\n",
    "                        # Use langdetect to identify the language\n",
    "                        try:\n",
    "                            language = detect(linked_page_soup.get_text())\n",
    "                        except:\n",
    "                            language = \"unknown\"\n",
    "\n",
    "                        # Include only English and Hindi content\n",
    "                        if language in ['en', 'hi']:\n",
    "                            # Update the language column based on detected language\n",
    "                            if language == 'en':\n",
    "                                language_text = 'English'\n",
    "                            else:\n",
    "                                language_text = 'Hindi'\n",
    "\n",
    "                            # Print details\n",
    "                            print(f\"Headline: {headline}\\nAuthor: {author}\\nDate: {date}\\nDescription Paragraphs:\\n{description_paragraphs}\\nLanguage: {language_text}\\n\")\n",
    "\n",
    "                            # Write to CSV file for Hindi and English news\n",
    "                            csv_writer_all.writerow({\n",
    "                                'Link Number': i,\n",
    "                                'Language': language_text,\n",
    "                                'News Link': news_link,\n",
    "                                'Description': description,\n",
    "                                'Headline': headline,\n",
    "                                'Author': author,\n",
    "                                'Date': date,\n",
    "                                'Paragraph': description_paragraphs\n",
    "                                \n",
    "                            })\n",
    "                        else:\n",
    "                            print(f\"Skipping Link {i} due to unsupported language: {language}\\n\")\n",
    "                    else:\n",
    "                        print(f\"Skipping Link {i} due to empty description paragraphs.\\n\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve linked page. Status Code: {linked_page_response.status_code}\\n\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Invalid URL. Skipping link {i}\\n\")\n",
    "\n",
    "    print(f\"\\nData has been appended to the CSV file: {csv_file_path_all}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db9a128-e86d-4009-ae3b-72a74e65c761",
   "metadata": {},
   "source": [
    "### Remove Ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863b0f74-98c3-48da-85e2-4c58358736fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from langdetect import detect\n",
    "\n",
    "def is_valid_url(url):\n",
    "    try:\n",
    "        result = urlparse(url)\n",
    "        return all([result.scheme, result.netloc])\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def is_non_empty_paragraph(p):\n",
    "    return p.text.strip() if p.text.strip() else None\n",
    "\n",
    "def extract_author(author_element):\n",
    "    if author_element.name == 'a':\n",
    "        return author_element.text.strip()\n",
    "    elif author_element.find('a'):\n",
    "        # Extract text from anchor tag inside the span\n",
    "        return author_element.find('a').text.strip()\n",
    "    else:\n",
    "        return \"N/A\"\n",
    "\n",
    "url = \"https://archives.ndtv.com/articles/2021-04.html\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    news_links = soup.find_all('a')\n",
    "\n",
    "    # Skip the first 40 links\n",
    "    start_index = 2000\n",
    "    news_links = news_links[start_index:]\n",
    "\n",
    "    # Limit the number of outputs\n",
    "    num_outputs_to_display = 4000\n",
    "\n",
    "    # CSV file setup for Hindi and English news\n",
    "    csv_file_path_all = \"news_data21_alls.csv\"\n",
    "    fieldnames = ['Link Number', 'News Link', 'Language', 'Description', 'Headline', 'Author', 'Date', 'Paragraph']\n",
    "\n",
    "    with open(csv_file_path_all, 'a', newline='', encoding='utf-8') as csv_file_all:\n",
    "        csv_writer_all = csv.DictWriter(csv_file_all, fieldnames=fieldnames)\n",
    "\n",
    "        # If the file is empty, write the header\n",
    "        if csv_file_all.tell() == 0:\n",
    "            csv_writer_all.writeheader()\n",
    "\n",
    "        for i, link in enumerate(news_links[:num_outputs_to_display], start=start_index + 1):\n",
    "            news_link = link.get('href')\n",
    "            description = link.text.strip()\n",
    "\n",
    "            # Check if the link is valid\n",
    "            if is_valid_url(news_link):\n",
    "                # Print the information\n",
    "                print(f\"Processing Link {i}: {news_link}\\nDescription: {description}\\n\")\n",
    "\n",
    "                # Retrieve details from the linked page\n",
    "                linked_page_response = requests.get(news_link)\n",
    "                if linked_page_response.status_code == 200:\n",
    "                    linked_page_soup = BeautifulSoup(linked_page_response.content, 'html.parser')\n",
    "\n",
    "                    # Extract details\n",
    "                    headline_element = linked_page_soup.find('h1', itemprop='headline')\n",
    "                    author_elements = linked_page_soup.find_all('span', itemprop='author')\n",
    "                    published_date = linked_page_soup.select_one('.pst-by_li:contains(\"Updated:\")')\n",
    "                    date = published_date.text.replace(\"Updated:\", \"\").strip() if published_date else \"N/A\"\n",
    "\n",
    "                    # Exclude paragraphs within the specified div with class \"ft-social\"\n",
    "                    excluded_div = linked_page_soup.find('div', class_='ft-social')\n",
    "                    excluded_paragraphs = set(excluded_div.find_all('p')) if excluded_div else set()\n",
    "\n",
    "                    # Limit the number of paragraphs for description\n",
    "                    description_paragraphs = [\n",
    "                        is_non_empty_paragraph(p)\n",
    "                        for p in linked_page_soup.find_all('p')\n",
    "                        if p not in excluded_paragraphs\n",
    "                        and \"Advertisement\" not in p.get_text(strip=True)  # Exclude Advertisement paragraphs\n",
    "                    ]\n",
    "\n",
    "                    if any(description_paragraphs):\n",
    "                        headline = headline_element.text.strip() if headline_element else \"N/A\"\n",
    "                        authors = [extract_author(author) for author in author_elements]\n",
    "                        author = ', '.join(authors) if authors else \"N/A\"\n",
    "\n",
    "                        # Use langdetect to identify the language\n",
    "                        try:\n",
    "                            language = detect(linked_page_soup.get_text())\n",
    "                        except:\n",
    "                            language = \"unknown\"\n",
    "\n",
    "                        # Include only English and Hindi content\n",
    "                        if language in ['en', 'hi']:\n",
    "                            # Update the language column based on detected language\n",
    "                            if language == 'en':\n",
    "                                language_text = 'English'\n",
    "                            else:\n",
    "                                language_text = 'Hindi'\n",
    "\n",
    "                            # Concatenate the description paragraphs into a single string\n",
    "                            description_text = ' '.join(filter(None, description_paragraphs))\n",
    "\n",
    "                            # Print details\n",
    "                            print(f\"Headline: {headline}\\nAuthor: {author}\\nDate: {date}\\nDescription Paragraphs:\\n{description_text}\\nLanguage: {language_text}\\n\")\n",
    "\n",
    "                            # Write to CSV file for Hindi and English news\n",
    "                            csv_writer_all.writerow({\n",
    "                                'Link Number': i,\n",
    "                                'Language': language_text,\n",
    "                                'News Link': news_link,\n",
    "                                'Description': description,\n",
    "                                'Headline': headline,\n",
    "                                'Author': author,\n",
    "                                'Date': date,\n",
    "                                'Paragraph': description_text\n",
    "                            })\n",
    "                        else:\n",
    "                            print(f\"Skipping Link {i} due to unsupported language: {language}\\n\")\n",
    "                    else:\n",
    "                        print(f\"Skipping Link {i} due to empty description paragraphs.\\n\")\n",
    "\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve linked page. Status Code: {linked_page_response.status_code}\\n\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Invalid URL. Skipping link {i}\\n\")\n",
    "\n",
    "    print(f\"\\nData has been appended to the CSV file: {csv_file_path_all}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcfd55c-6035-4bab-82ba-c44725262870",
   "metadata": {},
   "source": [
    "### Collecting news Through Selenium Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163a8fa-a26b-42e4-b943-7740dddd034f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "from dateutil import parser\n",
    "\n",
    "# Set up the ChromeOptions\n",
    "# Set up the Selenium webdriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "# Replace 'your_website_url' with the actual URL of your website\n",
    "website_url = 'https://www.pib.gov.in/indexd.aspx'\n",
    "# Replace 'your_website_url' with the actual URL of your website\n",
    "driver.get(website_url)\n",
    "\n",
    "# Select Hindi language\n",
    "language_dropdown = Select(driver.find_element(By.ID, 'Bar1_ddlLang'))\n",
    "language_dropdown.select_by_value('2')\n",
    "\n",
    "time.sleep(30)\n",
    "press_releases_link = driver.find_element(By.CSS_SELECTOR, 'ul li a[href=\"/PMContents/PMContents.aspx?menuid=1&Lang=2&RegionId=3\"]')\n",
    "press_releases_link.click()\n",
    "\n",
    "\n",
    "time.sleep(4)\n",
    "\n",
    "# Click on the first article link\n",
    "first_article_link = driver.find_element(By.CSS_SELECTOR, 'ul.num li a')\n",
    "first_article_link.click()\n",
    "\n",
    "time.sleep(8)\n",
    "\n",
    "# Select the desired date\n",
    "# Select the desired date (replace with your own logic if needed)\n",
    "day_dropdown = Select(driver.find_element(By.ID, 'ContentPlaceHolder1_ddlday'))\n",
    "day_dropdown.select_by_value('0')  # Replace '1' with the desired day\n",
    "\n",
    "# List of ministry values\n",
    "ministry_arr = [15, 1340, 40, 31, 53, 5, 47, 11, 1336, 21]\n",
    "\n",
    "# Create a CSV file and write the header\n",
    "csv_file_path = 'news/hindii_2023.csv'\n",
    "with open(csv_file_path, 'a', newline='', encoding='utf-8-sig') as csv_file: \n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['Link','Title','Language','Date', 'Content','Classifier'])\n",
    "    # Iterate through each month\n",
    "    for month_value in range(1, 13):\n",
    "        month_dropdown = Select(driver.find_element(By.ID, 'ContentPlaceHolder1_ddlMonth'))\n",
    "        month_dropdown.select_by_value(str(month_value))\n",
    "        # Re-locate the year dropdown after interacting with day and month dropdowns\n",
    "        year_dropdown = Select(driver.find_element(By.ID, 'ContentPlaceHolder1_ddlYear'))\n",
    "        year_dropdown.select_by_value('2023')  # Replace '2021' with the desired year\n",
    "\n",
    "        # Wait for the page to update with the selected month's news\n",
    "        time.sleep(4)  # Adjust the sleep duration based on your website's loading time\n",
    "\n",
    "        # Iterate through each ministry\n",
    "        for ministry_value in ministry_arr:\n",
    "            ministry_dropdown = Select(driver.find_element(By.ID, 'ContentPlaceHolder1_ddlMinistry'))\n",
    "            ministry_dropdown.select_by_value(str(ministry_value))\n",
    "\n",
    "            # Wait for the page to update with the selected ministry's news\n",
    "            time.sleep(4)  # Adjust the sleep duration based on your website's loading time\n",
    "\n",
    "            # Scraping news articles\n",
    "            try:\n",
    "                link_elements = driver.find_elements(By.CSS_SELECTOR, 'ul.leftul li a.listLeftrel2')\n",
    "            except:\n",
    "                link_elements = []\n",
    "\n",
    "            # Iterate through each link\n",
    "            for link_element in link_elements:\n",
    "                title = link_element.text\n",
    "                onclick_value = link_element.get_attribute('onclick').split('(')[-1].split(')')[0]\n",
    "\n",
    "\n",
    "                # Build the iframe URL\n",
    "                iframe_url = f'https://www.pib.gov.in/PressReleasePage.aspx?PRID={onclick_value}'\n",
    "                # Open a new tab/window and switch to it\n",
    "                driver.execute_script(\"window.open('', '_blank');\")\n",
    "                driver.switch_to.window(driver.window_handles[1])\n",
    "                # Open the iframe URL in the new tab/window\n",
    "                driver.get(iframe_url)\n",
    "                time.sleep(8)\n",
    "                # Find the input tag by ID\n",
    "                input_tag = driver.find_element(By.ID, 'ltrDescriptionn')\n",
    "                # Extract the value attribute text\n",
    "                value_text = input_tag.get_attribute('value')\n",
    "                # Parse the HTML content using BeautifulSoup\n",
    "                soup = BeautifulSoup(value_text, 'html.parser')\n",
    "                # Find all paragraphs in the HTML content\n",
    "                target_paragraphs = soup.find_all('p', text=lambda x: x and re.search(r'\\*\\s*\\*\\s*\\*', x))\n",
    "                date_element = driver.find_element(By.CSS_SELECTOR, 'div.ReleaseDateSubHeaddateTime')\n",
    "                # Extract the text content from the element\n",
    "                publish_date_text = date_element.text\n",
    "                \n",
    "                # Use dateutil.parser to parse the date\n",
    "                try:\n",
    "                    parsed_date = parser.parse(publish_date_text, fuzzy=True)\n",
    "                    formatted_date = parsed_date.strftime('%d-%m-%Y')\n",
    "                except ValueError:\n",
    "                    print(\"N/A\")\n",
    "                # Check if there are any target paragraphs\n",
    "                if target_paragraphs:\n",
    "                    # Find all previous siblings (preceding <p> tags) and concatenate their text content until the target paragraph\n",
    "                    concatenated_text = \"\"\n",
    "                    for target_paragraph in target_paragraphs:\n",
    "                        current_paragraph = target_paragraph.find_previous('p')\n",
    "\n",
    "                        while current_paragraph and not re.search(r'\\*\\s*\\*\\s*\\*', current_paragraph.text):\n",
    "                            concatenated_text = current_paragraph.text.strip() + \" \" + concatenated_text\n",
    "                            current_paragraph = current_paragraph.find_previous('p')\n",
    "\n",
    "                    csv_writer.writerow([iframe_url,title,\"Hindi\",formatted_date, concatenated_text.strip(),\"Government\"])\n",
    "                    \n",
    "                    #print(f\"Link: {title}\\n  Date :{formatted_date}\\nContent:{concatenated_text.strip()}\\n\")\n",
    "                \n",
    "                # Close the current tab/window and switch back to the main window\n",
    "                driver.close()\n",
    "                driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
